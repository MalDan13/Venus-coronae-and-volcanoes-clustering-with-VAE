{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089701fd-fd71-45af-8026-40f16ae4cfd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:57:29.879842Z",
     "iopub.status.busy": "2025-05-19T14:57:29.879182Z",
     "iopub.status.idle": "2025-05-19T14:57:46.039560Z",
     "shell.execute_reply": "2025-05-19T14:57:46.038836Z",
     "shell.execute_reply.started": "2025-05-19T14:57:29.879811Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from abc import abstractmethod\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as F1\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor, transforms\n",
    "from tqdm import tqdm\n",
    "from types_ import *\n",
    "from torchmetrics import MultiScaleStructuralSimilarityIndexMeasure\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim_skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b924002-e09c-4326-967c-b38f26604669",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:57:46.042116Z",
     "iopub.status.busy": "2025-05-19T14:57:46.040843Z",
     "iopub.status.idle": "2025-05-19T14:57:46.056552Z",
     "shell.execute_reply": "2025-05-19T14:57:46.055809Z",
     "shell.execute_reply.started": "2025-05-19T14:57:46.042074Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseVAE(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size: int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faeb9da3-7464-4cf8-864b-7b587ff96e2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:57:46.059114Z",
     "iopub.status.busy": "2025-05-19T14:57:46.057742Z",
     "iopub.status.idle": "2025-05-19T14:57:46.113771Z",
     "shell.execute_reply": "2025-05-19T14:57:46.113009Z",
     "shell.execute_reply.started": "2025-05-19T14:57:46.059074Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PairedTopoDataset(Dataset):\n",
    "    def __init__(self, image_dir, topo_dir, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Папка с обычными изображениями (без `_topo`).\n",
    "            topo_dir (str): Папка с топо-изображениями (с `_topo`).\n",
    "            transform (callable, optional): Дополнительные аугментации.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.topo_dir = topo_dir\n",
    "        self.transform = transform\n",
    "        self.to_tensor = ToTensor()  # Конвертирует PIL → Tensor и нормализует в [0, 1]\n",
    "\n",
    "        # Собираем все файлы и находим пары (image, topo)\n",
    "        self.pairs = self._find_paired_files()\n",
    "\n",
    "    def _extract_base_and_number(self, filename):\n",
    "        \"\"\"Извлекает базовое имя и номер из названия файла.\n",
    "        Пример:\n",
    "            \"cor_000_Pomona_..._0.png\" → (\"cor_000_Pomona_...\", 0)\n",
    "            \"cor_000_Pomona_..._topo_5.png\" → (\"cor_000_Pomona_...\", 5)\n",
    "        \"\"\"\n",
    "        # Удаляем расширение (.png, .jpg и т.д.)\n",
    "        name_without_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Разделяем имя и номер (последнее `_число`)\n",
    "        parts = name_without_ext.rsplit(\"_\", 1)\n",
    "        if len(parts) != 2:\n",
    "            return None, None  # Неправильный формат\n",
    "\n",
    "        base_part, number_str = parts\n",
    "        if not number_str.isdigit():\n",
    "            return None, None  # Последняя часть — не число\n",
    "\n",
    "        # Если в названии есть `_topo`, убираем его из базовой части\n",
    "        if \"_topo\" in base_part:\n",
    "            base_part = base_part.replace(\"_topo\", \"\")\n",
    "\n",
    "        return base_part, int(number_str)\n",
    "\n",
    "    def _find_paired_files(self):\n",
    "        \"\"\"Находит все пары (image, topo) с одинаковыми base_name и номером.\"\"\"\n",
    "        # Собираем все файлы из image_dir и topo_dir\n",
    "        image_files = os.listdir(self.image_dir)\n",
    "        topo_files = os.listdir(self.topo_dir)\n",
    "\n",
    "        # Создаем словарь: { (base_name, number) → {\"image\": path, \"topo\": path} }\n",
    "        pairs_dict = {}\n",
    "\n",
    "        # Обрабатываем обычные изображения (без `_topo`)\n",
    "        for img_file in image_files:\n",
    "            base, num = self._extract_base_and_number(img_file)\n",
    "            if base is None:\n",
    "                continue  # Пропускаем файлы с неправильным форматом\n",
    "            key = (base, num)\n",
    "            if key not in pairs_dict:\n",
    "                pairs_dict[key] = {\"image\": None, \"topo\": None}\n",
    "            pairs_dict[key][\"image\"] = os.path.join(self.image_dir, img_file)\n",
    "\n",
    "        # Обрабатываем топо-изображения (с `_topo`)\n",
    "        for topo_file in topo_files:\n",
    "            base, num = self._extract_base_and_number(topo_file)\n",
    "            if base is None:\n",
    "                continue\n",
    "            key = (base, num)\n",
    "            if key not in pairs_dict:\n",
    "                continue  # Нет пары в image_dir → пропускаем\n",
    "            pairs_dict[key][\"topo\"] = os.path.join(self.topo_dir, topo_file)\n",
    "\n",
    "        # Оставляем только полные пары (где есть и image, и topo)\n",
    "        valid_pairs = []\n",
    "        for key in pairs_dict:\n",
    "            if pairs_dict[key][\"image\"] and pairs_dict[key][\"topo\"]:\n",
    "                img_name = os.path.basename(pairs_dict[key][\"image\"])\n",
    "                topo_name = os.path.basename(pairs_dict[key][\"topo\"])\n",
    "                valid_pairs.append(\n",
    "                    (\n",
    "                        pairs_dict[key][\"image\"],  # путь к обычному изображению\n",
    "                        pairs_dict[key][\"topo\"],  # путь к топо-изображению\n",
    "                        img_name,  # имя файла обычного изображения\n",
    "                        topo_name,  # имя файла топо-изображения\n",
    "                    )\n",
    "                )\n",
    "        return valid_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, topo_path, img_name, topo_name = self.pairs[idx]\n",
    "\n",
    "#         # Загружаем изображения\n",
    "#         img = Image.open(img_path).convert(\"L\")\n",
    "#         topo = Image.open(topo_path).convert(\"L\")\n",
    "\n",
    "#         img = np.array(img).astype(np.float32) / 255\n",
    "#         topo = np.array(topo).astype(np.float32) / 255\n",
    "\n",
    "#         # Конвертируем в тензоры [1, H, W] с нормализацией [0, 1]\n",
    "#         img_tensor = self.to_tensor(img)  # размер [1, H, W]\n",
    "#         topo_tensor = self.to_tensor(topo)  # размер [1, H, W]\n",
    "\n",
    "#         # Объединяем по каналам → [2, H, W]\n",
    "#         combined = torch.cat((img_tensor, topo_tensor), dim=0)\n",
    "\n",
    "#         # Применяем аугментации\n",
    "#         if self.transform:\n",
    "#             combined = self.transform(combined)\n",
    "\n",
    "#         return topo_tensor, topo_name\n",
    "    \n",
    "    # Загружаем изображения\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        topo = Image.open(topo_path).convert(\"L\")\n",
    "\n",
    "        # Уменьшаем разрешение в 50 раз\n",
    "        new_size = (\n",
    "            int(img.width / 50),\n",
    "            int(img.height / 50),\n",
    "        )\n",
    "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "        topo = topo.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Конвертируем в numpy и нормализуем\n",
    "        img = np.array(img).astype(np.float32) / 255\n",
    "        topo = np.array(topo).astype(np.float32) / 255\n",
    "\n",
    "        # Преобразуем в тензоры [1, H, W]\n",
    "        img_tensor = self.to_tensor(img)\n",
    "        topo_tensor = self.to_tensor(topo)\n",
    "        \n",
    "        # Применяем аугментации\n",
    "        img_tensor = self.transform(img_tensor)\n",
    "        topo_tensor = self.transform(topo_tensor)\n",
    "\n",
    "#         # Объединяем по каналам → [2, H, W]\n",
    "#         combined = torch.cat((img_tensor, topo_tensor), dim=0)\n",
    "\n",
    "#         # Применяем аугментации\n",
    "#         if self.transform:\n",
    "#             combined = self.transform(combined)\n",
    "\n",
    "        return topo_tensor, topo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5782a690-2e5d-4985-a074-7cca52caf263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:57:46.117135Z",
     "iopub.status.busy": "2025-05-19T14:57:46.115750Z",
     "iopub.status.idle": "2025-05-19T14:57:46.179143Z",
     "shell.execute_reply": "2025-05-19T14:57:46.178302Z",
     "shell.execute_reply.started": "2025-05-19T14:57:46.117077Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VanillaVAE(BaseVAE):\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels: int, latent_dim: int, hidden_dims: List = None, **kwargs\n",
    "    ) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [16, 32, 64]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        out_channels=h_dim,\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "            # in_channels = 2\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1] * 2 * 2, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1] * 2 * 2, latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "               \n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 2 * 2)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        hidden_dims[i],\n",
    "                        hidden_dims[i + 1],\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        output_padding=1 if i < len(hidden_dims)-2 else 0\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                hidden_dims[-1],\n",
    "                hidden_dims[-1],\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                output_padding=0,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(hidden_dims[-1], out_channels=1, kernel_size=3, padding=1),\n",
    "            nn.Upsample(size=(10, 10), mode='bilinear')  # Добавляем финальный resize до 10×10\n",
    "        )\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        # print(f\"Encoder output shape: {result.shape}\")\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        # print(f\"Flattened shape: {result.shape}\")\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 64, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self, *args, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        def mssim_loss(recons, target):\n",
    "            return 1 - ms_ssim(recons, target, data_range=1.0, size_average=True)\n",
    "\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "        log_var = torch.clamp(log_var, min=-10, max=10)\n",
    "\n",
    "        kld_loss = torch.mean(\n",
    "            -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0\n",
    "        )\n",
    "        kld_weight = 0.0001\n",
    "\n",
    "        recons_loss = F.mse_loss(recons, input[:,0,:,:].unsqueeze(1))\n",
    "        recons_weight = 0.99\n",
    "\n",
    "        loss = recons_loss * recons_weight + kld_loss * kld_weight\n",
    "        \n",
    "        return {\"loss\": loss, \"Reconstruction_Loss\": recons_loss, \"KLD\": kld_loss}\n",
    "\n",
    "    def sample(self, num_samples: int, current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples, self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd2b816-71f4-4124-ad3e-5a0782142c93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:57:46.181342Z",
     "iopub.status.busy": "2025-05-19T14:57:46.180371Z",
     "iopub.status.idle": "2025-05-19T14:57:46.196186Z",
     "shell.execute_reply": "2025-05-19T14:57:46.195242Z",
     "shell.execute_reply.started": "2025-05-19T14:57:46.181301Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Определим аугментации\n",
    "class Random90Rotation:\n",
    "    \"\"\"Случайный поворот на угол, кратный 90 градусам (0, 90, 180, 270)\"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        angle = random.choice([0, 90, 180, 270])\n",
    "        return F1.rotate(img, angle)\n",
    "\n",
    "\n",
    "augmentations = transforms.Compose(\n",
    "    [\n",
    "        Random90Rotation(),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a79843f8-da3f-407c-8a0d-94580de07bda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:57:46.198556Z",
     "iopub.status.busy": "2025-05-19T14:57:46.197367Z",
     "iopub.status.idle": "2025-05-19T14:57:47.277425Z",
     "shell.execute_reply": "2025-05-19T14:57:47.276582Z",
     "shell.execute_reply.started": "2025-05-19T14:57:46.198516Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_dir=r\"/home/jupyter/datasphere/filestore/tyles/images\"\n",
    "topo_dir=r\"/home/jupyter/datasphere/filestore/tyles/topo\"\n",
    "    \n",
    "dataset = PairedTopoDataset(\n",
    "    image_dir,\n",
    "    topo_dir,\n",
    "    transform = augmentations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8b7d559-e2ec-4b93-bfb5-451cc8b85838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:57:47.278898Z",
     "iopub.status.busy": "2025-05-19T14:57:47.278181Z",
     "iopub.status.idle": "2025-05-19T14:57:47.488769Z",
     "shell.execute_reply": "2025-05-19T14:57:47.488003Z",
     "shell.execute_reply.started": "2025-05-19T14:57:47.278863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность изображения: torch.Size([1, 10, 10])\n",
      "Тип данных: torch.float32\n",
      "Имя файла: cor_000_Pomona_437x282km_79.3_-60.6_466.0km_topo_1.tif\n"
     ]
    }
   ],
   "source": [
    "# Выбираем первое изображение из датасета\n",
    "sample, img_name = dataset[0]  # Или sample = dataset[0][0] если возвращается (img, name)\n",
    "\n",
    "# Размерность изображения\n",
    "print(f\"Размерность изображения: {sample.shape}\")\n",
    "print(f\"Тип данных: {sample.dtype}\")\n",
    "print(f\"Имя файла: {img_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f15378c3-2afb-449f-b888-a26dee3c2689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T13:29:32.076978Z",
     "iopub.status.busy": "2025-05-19T13:29:32.076061Z",
     "iopub.status.idle": "2025-05-19T13:29:32.376927Z",
     "shell.execute_reply": "2025-05-19T13:29:32.375760Z",
     "shell.execute_reply.started": "2025-05-19T13:29:32.076938Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8404/1044423319.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m dataloader = torch.utils.data.DataLoader(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size, shuffle=True, num_workers=4\n",
    ")\n",
    "\n",
    "batch, img_names = next(iter(dataloader))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(4):\n",
    "    # Берем i-й пример из батча\n",
    "    img_pair = batch[i]\n",
    "\n",
    "    # Разделяем каналы\n",
    "    img = img_pair[0].numpy().squeeze()\n",
    "    # print(img.shape)\n",
    "\n",
    "    plt.subplot(4, 2, 2 * i + 1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(f\"{img_names[i]}\", fontsize=6)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c0de3a8-b145-463f-a3af-211ef28020da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T05:40:48.488152Z",
     "iopub.status.busy": "2025-05-16T05:40:48.487187Z",
     "iopub.status.idle": "2025-05-16T05:40:50.415365Z",
     "shell.execute_reply": "2025-05-16T05:40:50.414602Z",
     "shell.execute_reply.started": "2025-05-16T05:40:48.488117Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VanillaVAE(in_channels=1, latent_dim=16).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1f1ee3b-bc78-4738-a766-1a6d0ba072ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T05:40:57.994127Z",
     "iopub.status.busy": "2025-05-16T05:40:57.993073Z",
     "iopub.status.idle": "2025-05-16T05:41:50.186054Z",
     "shell.execute_reply": "2025-05-16T05:41:50.185197Z",
     "shell.execute_reply.started": "2025-05-16T05:40:57.994092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка весов\n",
    "weights_path = '/home/jupyter/datasphere/project/vanilla_vae_topo_weights50.pth'\n",
    "model.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477478e-d030-4be7-832b-75c3645c1f08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 21/32 [8:30:46<4:29:04, 1467.68s/it, Epoch Loss=0.1449, Recon=0.0117, KLD=106.3424, MSSIM=0.6248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights saved to 'vanilla_vae_weights110.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████▏ | 26/32 [10:31:40<2:25:22, 1453.74s/it, Epoch Loss=0.1448, Recon=0.0116, KLD=106.6614, MSSIM=0.6242]"
     ]
    }
   ],
   "source": [
    "# Создаем списки для хранения значений потерь\n",
    "train_loss_history = []\n",
    "recon_loss_history = []\n",
    "kld_loss_history = []\n",
    "mssim_loss_history = []\n",
    "\n",
    "# Общий прогресс по эпохам\n",
    "epoch_progress = tqdm(range(num_epochs), desc=\"Training\", position=0)\n",
    "\n",
    "# num_batches_to_process = 20\n",
    "    \n",
    "for epoch in epoch_progress:\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    epoch_recon_loss = 0\n",
    "    epoch_kld_loss = 0\n",
    "    epoch_mssim_loss = 0\n",
    "\n",
    "    # print(f\"Epoch {epoch + 1}\")\n",
    "    # i = 0\n",
    "    \n",
    "    # processed_batches = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # if processed_batches >= num_batches_to_process:\n",
    "        #     break\n",
    "    \n",
    "        x = batch[0].to(device)  # [B, 2, H, W]\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, input_batch, mu, logvar = model(x)\n",
    "        \n",
    "        loss_dict = model.loss_function(recon_batch, input_batch, mu, logvar)\n",
    "\n",
    "        loss = loss_dict[\"loss\"]\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Подрезка градиентов\n",
    "\n",
    "        # Проверка градиентов\n",
    "        # total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float('inf'))\n",
    "        # print(f\"Gradient norm: {total_norm}\")\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Собираем статистику по потерям\n",
    "        epoch_train_loss += loss.item()\n",
    "        epoch_recon_loss += loss_dict[\"Reconstruction_Loss\"].item()\n",
    "        epoch_kld_loss += loss_dict[\"KLD\"].item()\n",
    "        epoch_mssim_loss += loss_dict[\"MSSIM\"].item()\n",
    "        # processed_batches += 1\n",
    "\n",
    "        \n",
    "        # i += 1\n",
    "        # if i == 1:\n",
    "        #     break\n",
    "        \n",
    "#     # Нормализуем по количеству обработанных батчей\n",
    "#     epoch_train_loss /= num_batches_to_process\n",
    "#     epoch_kld_loss /= num_batches_to_process\n",
    "#     epoch_recon_loss /= num_batches_to_process\n",
    "#     epoch_mssim_loss /= num_batches_to_process\n",
    "    \n",
    "#     train_loss_history.append(epoch_train_loss)\n",
    "#     recon_loss_history.append(epoch_recon_loss)\n",
    "#     kld_loss_history.append(epoch_kld_loss)\n",
    "#     mssim_loss_history.append(epoch_mssim_loss)\n",
    "    \n",
    "#     epoch_progress.set_postfix({\n",
    "#         \"Epoch Loss\": f\"{epoch_train_loss:.4f}\",\n",
    "#         \"Recon\": f\"{epoch_recon_loss:.4f}\",\n",
    "#         \"KLD\": f\"{epoch_kld_loss:.4f}\",\n",
    "#         \"MSSIM\": f\"{epoch_mssim_loss:.4f}\",\n",
    "#     })\n",
    "\n",
    "    # Нормализуем по количеству батчей\n",
    "    num_batches = len(dataloader)\n",
    "    epoch_train_loss /= num_batches\n",
    "    epoch_recon_loss /= num_batches\n",
    "    epoch_kld_loss /= num_batches\n",
    "    epoch_mssim_loss /= num_batches\n",
    "\n",
    "    # Сохраняем значения для графиков\n",
    "    train_loss_history.append(epoch_train_loss)\n",
    "    recon_loss_history.append(epoch_recon_loss)\n",
    "    kld_loss_history.append(epoch_kld_loss)\n",
    "    mssim_loss_history.append(epoch_mssim_loss)\n",
    "\n",
    "    # Обновляем описание прогресса эпох\n",
    "    epoch_progress.set_postfix(\n",
    "        {\n",
    "            \"Epoch Loss\": f\"{epoch_train_loss:.4f}\",\n",
    "            \"Recon\": f\"{epoch_recon_loss:.4f}\",\n",
    "            \"KLD\": f\"{epoch_kld_loss:.4f}\",\n",
    "            \"MSSIM\": f\"{epoch_mssim_loss:.4f}\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Сохраняем веса модели в файл .pth\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), f\"vanilla_vae_topo_weights{epoch+90}.pth\")\n",
    "        print(f\"Weights saved to 'vanilla_vae_topo_weights{epoch+90}.pth'\")\n",
    "\n",
    "# Закрываем прогресс эпох\n",
    "epoch_progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41abaeb5-210f-4201-813f-3f16357c991f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Визуализация графиков потерь без KLD\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_loss_history, label=\"Total Loss\", linewidth=2)\n",
    "plt.plot(recon_loss_history, label=\"Reconstruction Loss (MSE)\", linestyle=\"--\")\n",
    "plt.plot(mssim_loss_history, label=\"MSSIM Loss\", linestyle=\"--\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3049894-ad52-4ed4-aa10-3348d03df886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Визуализация графика KLD\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(kld_loss_history, label=\"KLD Loss\", linestyle=\"--\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"KLD Loss History\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bfd665-3bc1-45ef-8caa-161a1ef0db28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0edd34d-c58e-413a-adeb-20a32dc7535f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_rmse(img1, img2):\n",
    "    return np.sqrt(np.mean((img1 - img2) ** 2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_images = 4\n",
    "    plt.figure(figsize=(14, 3 * num_images))\n",
    "    \n",
    "    # Инициализация списков для хранения метрик\n",
    "    ssim_values = []\n",
    "    psnr_values = []\n",
    "    rmse_values = []\n",
    "    max_diff_values = []\n",
    "    \n",
    "    random_indices = random.sample(range(len(dataset)), num_images)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        idx = random_indices[i]\n",
    "        x, img_name = dataset[idx]\n",
    "        x = x.unsqueeze(0).to(device)\n",
    "\n",
    "        recon_x, _, _, _ = model(x)\n",
    "        \n",
    "        original = x[0].cpu().numpy().squeeze()\n",
    "        reconstructed = recon_x[0].cpu().numpy().squeeze()\n",
    "        \n",
    "        if original.ndim == 3:\n",
    "            original_metric = original.transpose(1, 2, 0)\n",
    "            reconstructed_metric = reconstructed.transpose(1, 2, 0)\n",
    "        else:\n",
    "            original_metric = original\n",
    "            reconstructed_metric = reconstructed\n",
    "        \n",
    "        try:\n",
    "            ssim_val = ssim_skimage(original_metric, reconstructed_metric,\n",
    "                                  data_range=1.0, channel_axis=-1 if original_metric.ndim == 3 else None)\n",
    "            psnr_val = psnr(original_metric, reconstructed_metric, data_range=1.0)\n",
    "            rmse_val = calculate_rmse(original_metric, reconstructed_metric)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating metrics for {img_name}: {str(e)}\")\n",
    "            ssim_val, psnr_val, rmse_val = -1, -1, -1\n",
    "        \n",
    "        # Сохраняем значения метрик\n",
    "        ssim_values.append(ssim_val)\n",
    "        psnr_values.append(psnr_val)\n",
    "        rmse_values.append(rmse_val)\n",
    "        \n",
    "        # Визуализация\n",
    "        plt.subplot(num_images, 3, 3*i + 1)\n",
    "        plt.imshow(original[0] if original.ndim == 3 else original, cmap='gray', vmin=0, vmax=1)\n",
    "        plt.title(f\"Original: {img_name[:15]}...\", fontsize=8)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_images, 3, 3*i + 2)\n",
    "        plt.imshow(reconstructed[0] if reconstructed.ndim == 3 else reconstructed, cmap='gray', vmin=0, vmax=1)\n",
    "        plt.title(f\"Reconstructed\\nSSIM: {ssim_val:.3f}\\nPSNR: {psnr_val:.1f} dB\\nRMSE: {rmse_val:.3f}\", \n",
    "                 fontsize=8)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(num_images, 3, 3*i + 3)\n",
    "        difference = np.abs(\n",
    "            (original[0] if original.ndim == 3 else original) - \n",
    "            (reconstructed[0] if reconstructed.ndim == 3 else reconstructed))\n",
    "        plt.imshow(difference, cmap='hot', vmin=0, vmax=0.5)\n",
    "        plt.title(f\"Difference\\nMax: {difference.max():.3f}\", fontsize=8)\n",
    "        plt.colorbar(fraction=0.046, pad=0.04)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    # Вычисляем максимальную разницу для этого изображения\n",
    "        difference = np.abs(\n",
    "            (original[0] if original.ndim == 3 else original) - \n",
    "            (reconstructed[0] if reconstructed.ndim == 3 else reconstructed))\n",
    "        max_diff_values.append(difference.max())\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reconstructions_with_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Вычисляем средние значения метрик\n",
    "    avg_ssim = np.mean(ssim_values)\n",
    "    avg_psnr = np.mean(psnr_values)\n",
    "    avg_rmse = np.mean(rmse_values)\n",
    "    avg_max_diff = np.mean(max_diff_values)\n",
    "    \n",
    "    # Выводим средние значения\n",
    "    print(\"\\nСредние значения метрик по всем изображениям:\")\n",
    "    print(f\"SSIM: {avg_ssim:.4f}\")\n",
    "    print(f\"PSNR: {avg_psnr:.2f}\")\n",
    "    print(f\"RMSE: {avg_rmse:.4f}\")\n",
    "    print(f\"Максимальная разница: {avg_max_diff:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
