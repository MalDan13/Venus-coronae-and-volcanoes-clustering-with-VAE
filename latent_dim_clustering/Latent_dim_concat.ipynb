{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f44d46-2221-4e54-aa80-ece5f1367ee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:46:34.537343Z",
     "iopub.status.busy": "2025-05-28T11:46:34.536366Z",
     "iopub.status.idle": "2025-05-28T11:47:22.848418Z",
     "shell.execute_reply": "2025-05-28T11:47:22.847113Z",
     "shell.execute_reply.started": "2025-05-28T11:46:34.537303Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from abc import abstractmethod\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as F1\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor, transforms\n",
    "from tqdm import tqdm\n",
    "from types_ import *\n",
    "from torchmetrics import MultiScaleStructuralSimilarityIndexMeasure\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim_skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0656e13-3245-4dfc-acef-d2a34e50c898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:47:22.851267Z",
     "iopub.status.busy": "2025-05-28T11:47:22.850304Z",
     "iopub.status.idle": "2025-05-28T11:47:22.887723Z",
     "shell.execute_reply": "2025-05-28T11:47:22.886448Z",
     "shell.execute_reply.started": "2025-05-28T11:47:22.851216Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseVAE(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size: int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed2fdecf-cd4d-4b0f-8973-86d6b3c514ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:47:22.891318Z",
     "iopub.status.busy": "2025-05-28T11:47:22.889560Z",
     "iopub.status.idle": "2025-05-28T11:47:22.925928Z",
     "shell.execute_reply": "2025-05-28T11:47:22.924572Z",
     "shell.execute_reply.started": "2025-05-28T11:47:22.891177Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PairedTopoDatasetImg(Dataset):\n",
    "    def __init__(self, image_dir, topo_dir, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Папка с обычными изображениями (без `_topo`).\n",
    "            topo_dir (str): Папка с топо-изображениями (с `_topo`).\n",
    "            transform (callable, optional): Дополнительные аугментации.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.topo_dir = topo_dir\n",
    "        self.transform = transform\n",
    "        self.to_tensor = ToTensor()  # Конвертирует PIL → Tensor и нормализует в [0, 1]\n",
    "\n",
    "        # Собираем все файлы и находим пары (image, topo)\n",
    "        self.pairs = self._find_paired_files()\n",
    "\n",
    "    def _extract_base(self, filename):\n",
    "        \"\"\"Извлекает базовое имя и номер из названия файла.\n",
    "        Пример:\n",
    "            \"cor_000_Pomona_..._0.png\" → (\"cor_000_Pomona_...\", 0)\n",
    "            \"cor_000_Pomona_..._topo_5.png\" → (\"cor_000_Pomona_...\", 5)\n",
    "        \"\"\"\n",
    "        # Удаляем расширение (.png, .jpg и т.д.)\n",
    "        base_part = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Если в названии есть `_topo`, убираем его из базовой части\n",
    "        if \"_topo\" in base_part:\n",
    "            base_part = base_part.replace(\"_topo\", \"\")\n",
    "\n",
    "        return base_part\n",
    "\n",
    "    def _find_paired_files(self):\n",
    "        \"\"\"Находит все пары (image, topo) с одинаковыми base_name и номером.\"\"\"\n",
    "        # Собираем все файлы из image_dir и topo_dir\n",
    "        image_files = os.listdir(self.image_dir)\n",
    "        topo_files = os.listdir(self.topo_dir)\n",
    "\n",
    "        # Создаем словарь: { (base_name, number) → {\"image\": path, \"topo\": path} }\n",
    "        pairs_dict = {}\n",
    "\n",
    "        # Обрабатываем обычные изображения (без `_topo`)\n",
    "        for img_file in image_files:\n",
    "            base = self._extract_base(img_file)\n",
    "            if base is None:\n",
    "                continue  # Пропускаем файлы с неправильным форматом\n",
    "            key = base\n",
    "            if key not in pairs_dict:\n",
    "                pairs_dict[key] = {\"image\": None, \"topo\": None}\n",
    "            pairs_dict[key][\"image\"] = os.path.join(self.image_dir, img_file)\n",
    "\n",
    "        # Обрабатываем топо-изображения (с `_topo`)\n",
    "        for topo_file in topo_files:\n",
    "            base = self._extract_base(topo_file)\n",
    "            if base is None:\n",
    "                continue\n",
    "            key = base\n",
    "            if key not in pairs_dict:\n",
    "                continue  # Нет пары в image_dir → пропускаем\n",
    "            pairs_dict[key][\"topo\"] = os.path.join(self.topo_dir, topo_file)\n",
    "\n",
    "        # Оставляем только полные пары (где есть и image, и topo)\n",
    "        valid_pairs = []\n",
    "        for key in pairs_dict:\n",
    "            if pairs_dict[key][\"image\"] and pairs_dict[key][\"topo\"]:\n",
    "                img_name = os.path.basename(pairs_dict[key][\"image\"])\n",
    "                topo_name = os.path.basename(pairs_dict[key][\"topo\"])\n",
    "                valid_pairs.append(\n",
    "                    (\n",
    "                        pairs_dict[key][\"image\"],  # путь к обычному изображению\n",
    "                        pairs_dict[key][\"topo\"],  # путь к топо-изображению\n",
    "                        img_name,  # имя файла обычного изображения\n",
    "                        topo_name,  # имя файла топо-изображения\n",
    "                    )\n",
    "                )\n",
    "        return valid_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, topo_path, img_name, topo_name = self.pairs[idx]\n",
    "    \n",
    "    # Загружаем изображения\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        topo = Image.open(topo_path).convert(\"L\")\n",
    "\n",
    "        # Конвертируем в numpy и нормализуем\n",
    "        img = np.array(img).astype(np.float32) / 255\n",
    "        topo = np.array(topo).astype(np.float32) / 255\n",
    "\n",
    "        # Преобразуем в тензоры [1, H, W]\n",
    "        img_tensor = self.to_tensor(img)\n",
    "        topo_tensor = self.to_tensor(topo)\n",
    "        \n",
    "        # Применяем аугментации\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "            topo_tensor = self.transform(topo_tensor)\n",
    "\n",
    "        return img_tensor, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac0221d-052f-4eaf-af49-a1fc90946a1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:47:22.930894Z",
     "iopub.status.busy": "2025-05-28T11:47:22.928946Z",
     "iopub.status.idle": "2025-05-28T11:47:22.953984Z",
     "shell.execute_reply": "2025-05-28T11:47:22.952938Z",
     "shell.execute_reply.started": "2025-05-28T11:47:22.930856Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PairedTopoDatasetTopo(Dataset):\n",
    "    def __init__(self, image_dir, topo_dir, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Папка с обычными изображениями (без `_topo`).\n",
    "            topo_dir (str): Папка с топо-изображениями (с `_topo`).\n",
    "            transform (callable, optional): Дополнительные аугментации.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.topo_dir = topo_dir\n",
    "        self.transform = transform\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.pairs = self._find_paired_files()\n",
    "\n",
    "    def _extract_base_name(self, filename):\n",
    "        \"\"\"Извлекает базовое имя из названия файла (без _topo и расширения)\"\"\"\n",
    "        name = os.path.splitext(filename)[0]\n",
    "        return name.replace(\"_topo\", \"\")\n",
    "\n",
    "    def _find_paired_files(self):\n",
    "        \"\"\"Находит пары (image, topo) с одинаковыми базовыми именами\"\"\"\n",
    "        image_files = {self._extract_base_name(f): f for f in os.listdir(self.image_dir)}\n",
    "        topo_files = {self._extract_base_name(f): f for f in os.listdir(self.topo_dir)}\n",
    "        \n",
    "        # Находим общие базовые имена\n",
    "        common_bases = set(image_files.keys()) & set(topo_files.keys())\n",
    "        \n",
    "        # Создаем пары\n",
    "        valid_pairs = []\n",
    "        for base in common_bases:\n",
    "            img_path = os.path.join(self.image_dir, image_files[base])\n",
    "            topo_path = os.path.join(self.topo_dir, topo_files[base])\n",
    "            valid_pairs.append((\n",
    "                img_path,\n",
    "                topo_path,\n",
    "                image_files[base],  # имя обычного файла\n",
    "                topo_files[base]    # имя топо-файла\n",
    "            ))\n",
    "        \n",
    "        return valid_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, topo_path, img_name, topo_name = self.pairs[idx]\n",
    "        \n",
    "        # Загружаем изображения\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        topo = Image.open(topo_path).convert(\"L\")\n",
    "\n",
    "        # Уменьшаем разрешение в 50 раз\n",
    "        new_size = (int(img.width / 50), int(img.height / 50))\n",
    "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "        topo = topo.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Конвертируем в numpy и нормализуем\n",
    "        img = np.array(img).astype(np.float32) / 255\n",
    "        topo = np.array(topo).astype(np.float32) / 255\n",
    "\n",
    "        # Преобразуем в тензоры [1, H, W]\n",
    "        img_tensor = self.to_tensor(img)\n",
    "        topo_tensor = self.to_tensor(topo)\n",
    "        \n",
    "        # Применяем аугментации\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "            topo_tensor = self.transform(topo_tensor)\n",
    "\n",
    "        return topo_tensor, topo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d65ae9-20d8-4397-be72-063f608357a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:47:22.956724Z",
     "iopub.status.busy": "2025-05-28T11:47:22.955316Z",
     "iopub.status.idle": "2025-05-28T11:47:22.999299Z",
     "shell.execute_reply": "2025-05-28T11:47:22.998088Z",
     "shell.execute_reply.started": "2025-05-28T11:47:22.956669Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VanillaVAEImg(BaseVAE):\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels: int, latent_dim: int, hidden_dims: List = None, **kwargs\n",
    "    ) -> None:\n",
    "        super(VanillaVAEImg, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        out_channels=h_dim,\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "            # in_channels = 2\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1] * 16 * 16, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1] * 16 * 16, latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 16 * 16)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        hidden_dims[i],\n",
    "                        hidden_dims[i + 1],\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        output_padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                hidden_dims[-1],\n",
    "                hidden_dims[-1],\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                output_padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(hidden_dims[-1], out_channels=1, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        # print(f\"Encoder output shape: {result.shape}\")\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        # print(f\"Flattened shape: {result.shape}\")\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 16, 16)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self, *args, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        def mssim_loss(recons, target):\n",
    "            return 1 - ms_ssim(recons, target, data_range=1.0, size_average=True)\n",
    "\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "        log_var = torch.clamp(log_var, min=-10, max=10)\n",
    "\n",
    "        kld_loss = torch.mean(\n",
    "            -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0\n",
    "        )\n",
    "        kld_weight = 0.0001\n",
    "\n",
    "        recons_loss = F.mse_loss(recons, input[:,0,:,:].unsqueeze(1))\n",
    "        # recons_loss = F.mse_loss(recons, input[:,0,:,:])\n",
    "        recons_weight = 0.8\n",
    "        \n",
    "        mssim = mssim_loss(recons, input[:, 0, :, :].unsqueeze(1))\n",
    "        # mssim = mssim_loss(recons, input[:, 0, :, :])\n",
    "        mssim_weight = 1 - recons_weight\n",
    "\n",
    "        loss = recons_loss * recons_weight + kld_loss * kld_weight + mssim * mssim_weight\n",
    "        \n",
    "        # loss = recons_loss * recons_weight\n",
    "        return {\"loss\": loss, \"Reconstruction_Loss\": recons_loss, \"KLD\": kld_loss, \"MSSIM\": mssim}\n",
    "        # return {\"loss\": loss, \"Reconstruction_Loss\": recons_loss}\n",
    "\n",
    "    def sample(self, num_samples: int, current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples, self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3274571d-cbae-4e25-a845-3da72a79a8de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:47:23.003346Z",
     "iopub.status.busy": "2025-05-28T11:47:23.000924Z",
     "iopub.status.idle": "2025-05-28T11:47:23.037858Z",
     "shell.execute_reply": "2025-05-28T11:47:23.036724Z",
     "shell.execute_reply.started": "2025-05-28T11:47:23.003291Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VanillaVAETopo(BaseVAE):\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels: int, latent_dim: int, hidden_dims: List = None, **kwargs\n",
    "    ) -> None:\n",
    "        super(VanillaVAETopo, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [16, 32, 64]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        out_channels=h_dim,\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "            # in_channels = 2\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1] * 2 * 2, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1] * 2 * 2, latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "               \n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 2 * 2)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        hidden_dims[i],\n",
    "                        hidden_dims[i + 1],\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        output_padding=1 if i < len(hidden_dims)-2 else 0\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                hidden_dims[-1],\n",
    "                hidden_dims[-1],\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                output_padding=0,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(hidden_dims[-1], out_channels=1, kernel_size=3, padding=1),\n",
    "            nn.Upsample(size=(10, 10), mode='bilinear')  # Добавляем финальный resize до 10×10\n",
    "        )\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        # print(f\"Encoder output shape: {result.shape}\")\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        # print(f\"Flattened shape: {result.shape}\")\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 64, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self, *args, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        def mssim_loss(recons, target):\n",
    "            return 1 - ms_ssim(recons, target, data_range=1.0, size_average=True)\n",
    "\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "        log_var = torch.clamp(log_var, min=-10, max=10)\n",
    "\n",
    "        kld_loss = torch.mean(\n",
    "            -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0\n",
    "        )\n",
    "        kld_weight = 0.0001\n",
    "\n",
    "        recons_loss = F.mse_loss(recons, input[:,0,:,:].unsqueeze(1))\n",
    "        recons_weight = 0.99\n",
    "\n",
    "        loss = recons_loss * recons_weight + kld_loss * kld_weight\n",
    "        \n",
    "        return {\"loss\": loss, \"Reconstruction_Loss\": recons_loss, \"KLD\": kld_loss}\n",
    "\n",
    "    def sample(self, num_samples: int, current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples, self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab16d983-f15d-4463-80bb-7238a50a32a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:47:23.040036Z",
     "iopub.status.busy": "2025-05-28T11:47:23.039168Z",
     "iopub.status.idle": "2025-05-28T11:47:23.091899Z",
     "shell.execute_reply": "2025-05-28T11:47:23.090737Z",
     "shell.execute_reply.started": "2025-05-28T11:47:23.039982Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_dir=r\"/home/jupyter/datasphere/filestore/fulltiles/images\"  # папка с обычными изображениями\n",
    "topo_dir=r\"/home/jupyter/datasphere/filestore/fulltiles/topo\"  # папка с топо-изображениями\n",
    "    \n",
    "datasetImg = PairedTopoDatasetImg(\n",
    "    image_dir,\n",
    "    topo_dir,\n",
    "    transform = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d06b5c-439d-42ef-99f6-26b17c0fbbc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:47:23.095084Z",
     "iopub.status.busy": "2025-05-28T11:47:23.093427Z",
     "iopub.status.idle": "2025-05-28T11:47:23.121382Z",
     "shell.execute_reply": "2025-05-28T11:47:23.120225Z",
     "shell.execute_reply.started": "2025-05-28T11:47:23.095019Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size_Img = 4\n",
    "dataloaderImg = torch.utils.data.DataLoader(\n",
    "    datasetImg, batch_size_Img, shuffle=True, num_workers=8, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be1a090-f317-47f9-8daa-301515dc6f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:47:23.123888Z",
     "iopub.status.busy": "2025-05-28T11:47:23.122718Z",
     "iopub.status.idle": "2025-05-28T11:47:23.164680Z",
     "shell.execute_reply": "2025-05-28T11:47:23.163575Z",
     "shell.execute_reply.started": "2025-05-28T11:47:23.123832Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasetTopo = PairedTopoDatasetTopo(\n",
    "    image_dir,\n",
    "    topo_dir,\n",
    "    transform = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ea1104-acc0-4083-a08c-add453e29da4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:47:23.169693Z",
     "iopub.status.busy": "2025-05-28T11:47:23.168011Z",
     "iopub.status.idle": "2025-05-28T11:47:23.223305Z",
     "shell.execute_reply": "2025-05-28T11:47:23.222019Z",
     "shell.execute_reply.started": "2025-05-28T11:47:23.169599Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size_Topo = 4\n",
    "dataloaderTopo = torch.utils.data.DataLoader(\n",
    "    datasetTopo, batch_size_Topo, shuffle=True, num_workers=8, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df48feda-a185-40e5-8608-d6523fbf8616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:47:23.225377Z",
     "iopub.status.busy": "2025-05-28T11:47:23.224480Z",
     "iopub.status.idle": "2025-05-28T11:47:25.886517Z",
     "shell.execute_reply": "2025-05-28T11:47:25.885404Z",
     "shell.execute_reply.started": "2025-05-28T11:47:23.225299Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelImg = VanillaVAEImg(in_channels=1, latent_dim=512).to(device)\n",
    "optimizer = torch.optim.Adam(modelImg.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d2a0b8d-1ff8-4811-866a-601a68302350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:47:25.889590Z",
     "iopub.status.busy": "2025-05-28T11:47:25.887928Z",
     "iopub.status.idle": "2025-05-28T11:47:25.927681Z",
     "shell.execute_reply": "2025-05-28T11:47:25.926498Z",
     "shell.execute_reply.started": "2025-05-28T11:47:25.889509Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelTopo = VanillaVAETopo(in_channels=1, latent_dim=16).to(device)\n",
    "optimizer = torch.optim.Adam(modelTopo.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6b500ad-46e6-4489-8ba7-6294adb9ef83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:47:25.929638Z",
     "iopub.status.busy": "2025-05-28T11:47:25.928880Z",
     "iopub.status.idle": "2025-05-28T11:48:18.023852Z",
     "shell.execute_reply": "2025-05-28T11:48:18.022705Z",
     "shell.execute_reply.started": "2025-05-28T11:47:25.929587Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_path_Img = '/home/jupyter/datasphere/project/vanilla_vae_weights120.pth'\n",
    "modelImg.load_state_dict(torch.load(weights_path_Img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5993cef9-ef12-46d3-846b-1e5325c85786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:48:18.026816Z",
     "iopub.status.busy": "2025-05-28T11:48:18.025369Z",
     "iopub.status.idle": "2025-05-28T11:48:18.073175Z",
     "shell.execute_reply": "2025-05-28T11:48:18.071823Z",
     "shell.execute_reply.started": "2025-05-28T11:48:18.026723Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_path_Topo = '/home/jupyter/datasphere/project/vanilla_vae_topo_weights50.pth'\n",
    "modelTopo.load_state_dict(torch.load(weights_path_Topo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb7843d0-4b98-41b8-a1b5-593a28e29283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:48:18.075733Z",
     "iopub.status.busy": "2025-05-28T11:48:18.074576Z",
     "iopub.status.idle": "2025-05-28T11:48:18.094857Z",
     "shell.execute_reply": "2025-05-28T11:48:18.093757Z",
     "shell.execute_reply.started": "2025-05-28T11:48:18.075660Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaVAEImg(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=131072, out_features=512, bias=True)\n",
       "  (fc_var): Linear(in_features=131072, out_features=512, bias=True)\n",
       "  (decoder_input): Linear(in_features=512, out_features=131072, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelImg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0685f38-4f1a-4cbb-b0f6-1195023ca0f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:48:18.097668Z",
     "iopub.status.busy": "2025-05-28T11:48:18.095996Z",
     "iopub.status.idle": "2025-05-28T11:48:18.121624Z",
     "shell.execute_reply": "2025-05-28T11:48:18.120473Z",
     "shell.execute_reply.started": "2025-05-28T11:48:18.097613Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaVAETopo(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (fc_var): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (decoder_input): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Upsample(size=(10, 10), mode='bilinear')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTopo.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09255940-12f4-4d45-be01-b3d245dd9528",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:48:18.124771Z",
     "iopub.status.busy": "2025-05-28T11:48:18.123052Z",
     "iopub.status.idle": "2025-05-28T11:48:18.156148Z",
     "shell.execute_reply": "2025-05-28T11:48:18.155003Z",
     "shell.execute_reply.started": "2025-05-28T11:48:18.124733Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_large_image_with_overlap(model, large_img, tile_size=256, overlap=128, device='cuda'):\n",
    "    \"\"\"\n",
    "    Обрабатывает большое изображение тайлами с перекрытием\n",
    "    tile_size: размер тайла (рекомендуется 256-512 для больших изображений)\n",
    "    overlap: рекомендуемое значение - половина tile_size\n",
    "    \"\"\"\n",
    "    # Проверяем размерности\n",
    "    if len(large_img.shape) == 2:\n",
    "        large_img = large_img.unsqueeze(0).unsqueeze(0)\n",
    "    elif len(large_img.shape) == 3:\n",
    "        large_img = large_img.unsqueeze(0)\n",
    "    \n",
    "    h, w = large_img.shape[-2:]\n",
    "    stride = tile_size - overlap\n",
    "    \n",
    "    # Добавляем паддинг\n",
    "    pad_h = (tile_size - h % stride) if h % stride != 0 else 0\n",
    "    pad_w = (tile_size - w % stride) if w % stride != 0 else 0\n",
    "    \n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        large_img = F.pad(large_img, (0, pad_w, 0, pad_h), mode='reflect')\n",
    "    \n",
    "    # Создаем гауссову маску для плавного смешивания\n",
    "    y, x = torch.meshgrid(torch.linspace(-1, 1, tile_size),\n",
    "                      torch.linspace(-1, 1, tile_size),\n",
    "                      indexing='ij')\n",
    "    mask = torch.exp(-(x**2 + y**2))\n",
    "    mask = mask.to(device)\n",
    "    \n",
    "    # Инициализируем результат\n",
    "    result = torch.zeros_like(large_img, device=device)\n",
    "    count = torch.zeros_like(large_img, device=device)\n",
    "    \n",
    "    # Обрабатываем тайлы\n",
    "    for y in range(0, h + pad_h - tile_size + 1, stride):\n",
    "        for x in range(0, w + pad_w - tile_size + 1, stride):\n",
    "            tile = large_img[..., y:y+tile_size, x:x+tile_size].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                recon_tile = model(tile)[0].clamp(0, 1)  # Ограничиваем диапазон\n",
    "            \n",
    "            result[..., y:y+tile_size, x:x+tile_size] += recon_tile * mask\n",
    "            count[..., y:y+tile_size, x:x+tile_size] += mask\n",
    "    \n",
    "    # Нормализуем и обрезаем паддинг\n",
    "    result = (result / count)[..., :h, :w]\n",
    "    \n",
    "    return result.squeeze().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13bc0b6e-6d50-4eb5-b58b-921d2ee11099",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:48:18.159529Z",
     "iopub.status.busy": "2025-05-28T11:48:18.157466Z",
     "iopub.status.idle": "2025-05-28T11:48:18.189228Z",
     "shell.execute_reply": "2025-05-28T11:48:18.188059Z",
     "shell.execute_reply.started": "2025-05-28T11:48:18.159470Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_large_topo_with_overlap(model, large_img, tile_size=10, overlap=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Обрабатывает большое изображение тайлами с перекрытием\n",
    "    tile_size: размер тайла (квадратный)\n",
    "    overlap: величина перекрытия между тайлами (рекомендуется tile_size//2)\n",
    "    \"\"\"\n",
    "    # Проверяем размерности\n",
    "    if len(large_img.shape) == 2:\n",
    "        large_img = large_img.unsqueeze(0).unsqueeze(0)\n",
    "    elif len(large_img.shape) == 3:\n",
    "        large_img = large_img.unsqueeze(0)\n",
    "    \n",
    "    h, w = large_img.shape[-2:]\n",
    "    stride = tile_size - overlap\n",
    "    \n",
    "    # Добавляем паддинг чтобы покрыть все изображение\n",
    "    pad_h = (tile_size - h % stride) if h % stride != 0 else 0\n",
    "    pad_w = (tile_size - w % stride) if w % stride != 0 else 0\n",
    "    \n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        large_img = F.pad(large_img, (0, pad_w, 0, pad_h), mode='reflect')\n",
    "    \n",
    "    # Создаем маску для взвешенного сложения\n",
    "    mask = torch.ones((1, 1, tile_size, tile_size), device=device)\n",
    "    mask = F.pad(mask, (overlap//2, overlap//2, overlap//2, overlap//2), value=1e-6)\n",
    "    mask = mask[:, :, :tile_size, :tile_size]\n",
    "    \n",
    "    # Инициализируем результат и счетчик перекрытий\n",
    "    result = torch.zeros_like(large_img, device=device)\n",
    "    count = torch.zeros_like(large_img, device=device)\n",
    "    \n",
    "    # Собираем координаты тайлов\n",
    "    coords = []\n",
    "    for y in range(0, h + pad_h - tile_size + 1, stride):\n",
    "        for x in range(0, w + pad_w - tile_size + 1, stride):\n",
    "            coords.append((y, x))\n",
    "    \n",
    "    # Обрабатываем тайлы\n",
    "    for i in range(0, len(coords), 32):  # Пакетами по 32 тайла\n",
    "        batch_coords = coords[i:i+32]\n",
    "        tiles = []\n",
    "        for y, x in batch_coords:\n",
    "            tile = large_img[..., y:y+tile_size, x:x+tile_size]\n",
    "            tiles.append(tile)\n",
    "        \n",
    "        tiles = torch.cat(tiles, dim=0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            recon_tiles = model(tiles)[0]\n",
    "        \n",
    "        # Добавляем обработанные тайлы в результат с учетом маски\n",
    "        for j, (y, x) in enumerate(batch_coords):\n",
    "            result[..., y:y+tile_size, x:x+tile_size] += recon_tiles[j] * mask\n",
    "            count[..., y:y+tile_size, x:x+tile_size] += mask\n",
    "    \n",
    "    # Нормализуем результат\n",
    "    result = result / count\n",
    "    \n",
    "    # Убираем паддинг если добавляли\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        result = result[..., :h, :w]\n",
    "    \n",
    "    return result.squeeze().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ffed656-63eb-42c7-bb26-ec6a99eb0ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:48:18.191520Z",
     "iopub.status.busy": "2025-05-28T11:48:18.190384Z",
     "iopub.status.idle": "2025-05-28T11:55:59.031644Z",
     "shell.execute_reply": "2025-05-28T11:55:59.030391Z",
     "shell.execute_reply.started": "2025-05-28T11:48:18.191467Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 172/172 [07:23<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "def extract_latent_features_img(autoencoder, dataloader, device=device):\n",
    "    features = []\n",
    "    filenames = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, img_name in tqdm(dataloader, desc=\"Processing images\"):\n",
    "            img = img.to(device)\n",
    "            latent = process_large_image_with_overlap(autoencoder, img, tile_size=512, overlap=256, device=device)\n",
    "            latent = latent.cpu().to(torch.float16)\n",
    "            features.append(latent.cpu())\n",
    "            filenames.extend(img_name)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return torch.cat(features), filenames\n",
    "\n",
    "featuresImg, filenamesImg = extract_latent_features_img(modelImg, dataloaderImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d7ee861-8561-497c-bdcb-07aed18221fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:55:59.034612Z",
     "iopub.status.busy": "2025-05-28T11:55:59.033274Z",
     "iopub.status.idle": "2025-05-28T11:57:08.467251Z",
     "shell.execute_reply": "2025-05-28T11:57:08.465912Z",
     "shell.execute_reply.started": "2025-05-28T11:55:59.034571Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topo: 100%|██████████| 172/172 [01:09<00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_latent_features_topo(autoencoder, dataloader, device=device):\n",
    "    features = []\n",
    "    filenames = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for topo, topo_name in tqdm(dataloader, desc=\"Processing topo\"):\n",
    "            topo = topo.to(device)\n",
    "            latent = process_large_topo_with_overlap(autoencoder, topo, tile_size=10, overlap=5, device=device)\n",
    "            latent = latent.cpu().to(torch.float16)\n",
    "            features.append(latent.cpu())\n",
    "            filenames.extend(topo_name)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return torch.cat(features), filenames\n",
    "\n",
    "featuresTopo, filenamesTopo = extract_latent_features_topo(modelTopo, dataloaderTopo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdd1aa9a-acb0-4f66-b863-1e8540daccc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:57:08.470362Z",
     "iopub.status.busy": "2025-05-28T11:57:08.469528Z",
     "iopub.status.idle": "2025-05-28T11:57:08.504563Z",
     "shell.execute_reply": "2025-05-28T11:57:08.503482Z",
     "shell.execute_reply.started": "2025-05-28T11:57:08.470323Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_name(topo_name):\n",
    "    return topo_name.replace(\"_topo\", \"\")\n",
    "\n",
    "for i in range(len(filenamesTopo)):\n",
    "    filenamesTopo[i] = get_img_name(filenamesTopo[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67ed019e-4775-4d27-bff3-d02cb21cf91b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T11:58:28.246950Z",
     "iopub.status.busy": "2025-05-28T11:58:28.245965Z",
     "iopub.status.idle": "2025-05-28T11:58:33.493502Z",
     "shell.execute_reply": "2025-05-28T11:58:33.492134Z",
     "shell.execute_reply.started": "2025-05-28T11:58:28.246915Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 230/230 [00:04<00:00, 49.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed and saved to latent_features_expanded.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Определяем размер чанка (настройте под вашу систему)\n",
    "CHUNK_SIZE = (featuresImg.shape[1] + featuresTopo.shape[1]) // 1000\n",
    "\n",
    "# Создаем словари с numpy массивами\n",
    "latent_dictImg = {name: feat.numpy() for name, feat in zip(filenamesImg, featuresImg)}\n",
    "latent_dictTopo = {name: feat.numpy() for name, feat in zip(filenamesTopo, featuresTopo)}\n",
    "\n",
    "# Заголовки столбцов (создаем заранее)\n",
    "sar_columns = [f'features_SAR_{i}' for i in range(featuresImg.shape[1])]\n",
    "topo_columns = [f'features_topo_{i}' for i in range(featuresTopo.shape[1])]\n",
    "fieldnames = ['filename'] + sar_columns + topo_columns\n",
    "\n",
    "# Открываем CSV для постепенной записи\n",
    "with open('latent_features_expanded.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Обрабатываем данные чанками\n",
    "    for i in tqdm(range(0, len(filenamesImg), CHUNK_SIZE), desc=\"Processing chunks\"):\n",
    "        chunk_data = []\n",
    "        \n",
    "        for img_name in filenamesImg[i:i+CHUNK_SIZE]:\n",
    "            if img_name in latent_dictTopo:\n",
    "                sar_features = latent_dictImg[img_name].flatten()\n",
    "                topo_features = latent_dictTopo[img_name].flatten()\n",
    "                \n",
    "                record = {'filename': img_name}\n",
    "                record.update(zip(sar_columns, sar_features))\n",
    "                record.update(zip(topo_columns, topo_features))\n",
    "                \n",
    "                writer.writerow(record)\n",
    "\n",
    "print(\"Processing completed and saved to latent_features_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "473d9cbf-f931-46e0-8e17-e159017b2e99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T18:17:41.040173Z",
     "iopub.status.busy": "2025-05-28T18:17:41.038929Z",
     "iopub.status.idle": "2025-05-28T18:17:44.242268Z",
     "shell.execute_reply": "2025-05-28T18:17:44.241257Z",
     "shell.execute_reply.started": "2025-05-28T18:17:41.040120Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Файл успешно дополнен, отсутствующие столбцы заполнены нулями!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Загружаем основной файл\n",
    "df_main = pd.read_csv('latent_features_expanded.csv')\n",
    "\n",
    "# 2. Извлекаем ID из столбца filename\n",
    "df_main['uniq_ID'] = df_main['filename'].str.extract(r'^(cor_\\d+|vlc_\\d+|vlcInt_\\d+)')\n",
    "\n",
    "# 3. Загружаем и объединяем дополнительные файлы\n",
    "df_extra_combined = pd.DataFrame()\n",
    "required_columns = ['uniq_ID', 'Latitude', 'Longitude', 'Max_D_km', 'Min_D_km']  # Обязательные столбцы\n",
    "\n",
    "for file in Path('/home/jupyter/datasphere/project/Volc_centers_data/').glob('*.csv'):\n",
    "    df_temp = pd.read_csv(file)\n",
    "    \n",
    "    # Проверяем, есть ли все нужные столбцы. Если нет — добавляем и заполняем нулями.\n",
    "    for col in required_columns:\n",
    "        if col not in df_temp.columns:\n",
    "            df_temp[col] = 0\n",
    "    \n",
    "    df_extra_combined = pd.concat([df_extra_combined, df_temp], ignore_index=True)\n",
    "\n",
    "# 4. Объединяем с основным DataFrame (left join)\n",
    "df_main = pd.merge(\n",
    "    df_main,\n",
    "    df_extra_combined[required_columns],\n",
    "    on='uniq_ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 5. Удаляем временный столбец 'ID' (если не нужен)\n",
    "df_main.drop('uniq_ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d075cfd-22c5-4f6d-bd52-27aab65b2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для извлечения типа файла\n",
    "def extract_type(filename):\n",
    "    parts = str(filename).split('_', 1)  # Разделить только один раз по первому \"_\"\n",
    "    return parts[0]\n",
    "\n",
    "# Применение функции к столбцу filename\n",
    "df_main['type'] = df_main['filename'].apply(extract_type)\n",
    "\n",
    "# Переставляем столбцы так, чтобы 'type' шёл сразу после 'filename'\n",
    "cols = ['filename', 'type'] + [col for col in data.columns if col not in ['filename', 'type']]\n",
    "df_main = df_main[cols]\n",
    "\n",
    "# Просмотр первых строк\n",
    "print(df_main.head())\n",
    "\n",
    "file_name = 'latent_features_expanded_with_type.csv'\n",
    "\n",
    "# Сохранение в новый файл\n",
    "df_main.to_csv(file_name, index=False)\n",
    "\n",
    "print(f\"Датасет успешно создан и сохранен в файл {file_name}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
